{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-10T13:55:15.285810Z","iopub.execute_input":"2022-05-10T13:55:15.286121Z","iopub.status.idle":"2022-05-10T13:55:15.316653Z","shell.execute_reply.started":"2022-05-10T13:55:15.286043Z","shell.execute_reply":"2022-05-10T13:55:15.315945Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport nltk\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords\n\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\nfrom keras.models import Model\nimport gensim.downloader as api\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, Sequential\n\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.metrics import classification_report, f1_score, confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, learning_curve\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:58:38.846780Z","iopub.execute_input":"2022-05-10T13:58:38.847620Z","iopub.status.idle":"2022-05-10T13:58:38.859945Z","shell.execute_reply.started":"2022-05-10T13:58:38.847576Z","shell.execute_reply":"2022-05-10T13:58:38.859130Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\nprint(f\"Train data shape: {train_df.shape}\\nTest data shape: {test_df.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:13.381538Z","iopub.execute_input":"2022-05-10T13:56:13.381797Z","iopub.status.idle":"2022-05-10T13:56:13.449510Z","shell.execute_reply.started":"2022-05-10T13:56:13.381767Z","shell.execute_reply":"2022-05-10T13:56:13.448135Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# more information about columns \nprint(train_df.info())","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:15.453519Z","iopub.execute_input":"2022-05-10T13:56:15.454364Z","iopub.status.idle":"2022-05-10T13:56:15.480615Z","shell.execute_reply.started":"2022-05-10T13:56:15.454315Z","shell.execute_reply":"2022-05-10T13:56:15.479887Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(test_df.info())","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:18.518507Z","iopub.execute_input":"2022-05-10T13:56:18.518770Z","iopub.status.idle":"2022-05-10T13:56:18.533907Z","shell.execute_reply.started":"2022-05-10T13:56:18.518740Z","shell.execute_reply":"2022-05-10T13:56:18.532886Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# number of missing values per column in train \ntrain_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:21.228485Z","iopub.execute_input":"2022-05-10T13:56:21.229072Z","iopub.status.idle":"2022-05-10T13:56:21.240882Z","shell.execute_reply.started":"2022-05-10T13:56:21.229030Z","shell.execute_reply":"2022-05-10T13:56:21.240161Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# number of missing values per column in test\ntest_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:21.499360Z","iopub.execute_input":"2022-05-10T13:56:21.499570Z","iopub.status.idle":"2022-05-10T13:56:21.508330Z","shell.execute_reply.started":"2022-05-10T13:56:21.499545Z","shell.execute_reply":"2022-05-10T13:56:21.507593Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# appending the test data to train \nfull_df = pd.concat(objs=[train_df, test_df], axis=0)\nfull_df.head(10) # train data would be above the test data","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:23.614044Z","iopub.execute_input":"2022-05-10T13:56:23.614817Z","iopub.status.idle":"2022-05-10T13:56:23.638267Z","shell.execute_reply.started":"2022-05-10T13:56:23.614772Z","shell.execute_reply":"2022-05-10T13:56:23.637294Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# counting the number of positive and negative tweets\nsns.set(style=\"whitegrid\", color_codes=True)\nsns.catplot(\"target\", data=full_df, kind=\"count\", height=8)\nplt.title(\"Distribution of Target Counts\", size=20, weight=\"bold\")\nplt.xlabel(\"Target Label\", size=14, weight=\"bold\")\nplt.ylabel(\"Counts\", size=14, weight=\"bold\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:28.742116Z","iopub.execute_input":"2022-05-10T13:56:28.742789Z","iopub.status.idle":"2022-05-10T13:56:29.145709Z","shell.execute_reply.started":"2022-05-10T13:56:28.742738Z","shell.execute_reply":"2022-05-10T13:56:29.145028Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# creating a new dataframe for missing values percentage\nmissing_values = dict(zip([col+\"_missing_percent\" for col in full_df.columns if col != \"target\"],\n                          [round(100*full_df[col].isnull().sum()/len(full_df), 2) for col in full_df.columns\n                           if col != \"target\"]))\nmissing_values_df = pd.DataFrame(missing_values, index=[0])\nmissing_values_df = missing_values_df.melt(var_name= \"columns\", value_name= \"percentage\")\n\n# plotting missing values chart\nplt.figure(figsize=(10, 8))\nsns.set(style=\"whitegrid\", color_codes=True)\nseaborn_plot = sns.barplot(x=\"columns\", y=\"percentage\", data=missing_values_df)\nfor p in seaborn_plot.patches:\n    seaborn_plot.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center',\n                   va = 'center', xytext = (0, 9), textcoords = 'offset points')\nplt.title(\"Percentage of Missing Values in Columns\", size=20, weight=\"bold\")\nplt.xlabel(\"Columns\", size=14, weight=\"bold\")\nplt.ylabel(\"Percentage\", size=14, weight=\"bold\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:29.184929Z","iopub.execute_input":"2022-05-10T13:56:29.185148Z","iopub.status.idle":"2022-05-10T13:56:29.410494Z","shell.execute_reply.started":"2022-05-10T13:56:29.185122Z","shell.execute_reply":"2022-05-10T13:56:29.409818Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"plt.subplots(1,2,figsize=(14,4))\nplt.subplot(1,2,1); plt.hist(train_df['text'].str.len(), color='orange', alpha=0.6); plt.ylabel(''); plt.title('Words in sentences (train)')\nplt.subplot(1,2,2); plt.hist(test_df['text'].str.len(), color='green', alpha=0.6); plt.ylabel(''); plt.title('Words in sentences (test)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:32.150572Z","iopub.execute_input":"2022-05-10T13:56:32.151125Z","iopub.status.idle":"2022-05-10T13:56:32.543072Z","shell.execute_reply.started":"2022-05-10T13:56:32.151087Z","shell.execute_reply":"2022-05-10T13:56:32.541290Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# getting all the keywords from keyword column\nall_keywords = \" \".join([keyword for keyword in full_df[\"keyword\"].dropna()])\n\n# visualizing keywords\nword_cloud= WordCloud(width=800,\n                      height=500,\n                      max_font_size=112,\n                      background_color ='white',\n                      random_state=24).generate(all_keywords)\n\n# plotting \nplt.figure(figsize=(10, 8))\nplt.title(\"All keywords in the data\", size=20, weight=\"bold\")\nplt.imshow(word_cloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:32.544938Z","iopub.execute_input":"2022-05-10T13:56:32.545206Z","iopub.status.idle":"2022-05-10T13:56:33.534557Z","shell.execute_reply.started":"2022-05-10T13:56:32.545167Z","shell.execute_reply":"2022-05-10T13:56:33.533902Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# all keywords from tweets associated to a disaster \nall_disaster_keywords = \" \".join([keyword for keyword in full_df[full_df[\"target\"]==1][\"keyword\"].dropna()])\n\n# visualizing keywords\nword_cloud= WordCloud(width=800,\n                      height=500,\n                      max_font_size=112,\n                      background_color ='white',\n                      random_state=24).generate(all_disaster_keywords)\n\n# plotting \nplt.figure(figsize=(10, 8))\nplt.title(\"All keywords associated to Disasters\", size=20, weight=\"bold\")\nplt.imshow(word_cloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:33.535723Z","iopub.execute_input":"2022-05-10T13:56:33.536005Z","iopub.status.idle":"2022-05-10T13:56:34.474738Z","shell.execute_reply.started":"2022-05-10T13:56:33.535963Z","shell.execute_reply":"2022-05-10T13:56:34.474062Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# all keywords from tweets NOT associated to disaster\nall_non_disaster_keywords = \" \".join([keyword for keyword in full_df[full_df[\"target\"]==0][\"keyword\"].dropna()])\n\n# visualizing keywords\nword_cloud= WordCloud(width=800,\n                      height=500,\n                      max_font_size=112,\n                      background_color ='white',\n                      random_state=24).generate(all_non_disaster_keywords)\n\n# plotting \nplt.figure(figsize=(10, 8))\nplt.title(\"All keywords associated to Non-Disasters\", size=20, weight=\"bold\")\nplt.imshow(word_cloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:34.476454Z","iopub.execute_input":"2022-05-10T13:56:34.476873Z","iopub.status.idle":"2022-05-10T13:56:35.403627Z","shell.execute_reply.started":"2022-05-10T13:56:34.476836Z","shell.execute_reply":"2022-05-10T13:56:35.402192Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Text Cleaning \ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+')\n    return url.sub(r' httpsmark ', text)\n\ndef remove_hashtag(text):\n    hashtag = re.compile(r'#')\n    return hashtag.sub(r' hashtag ', text)\n\ndef remove_exclamation(text):\n    exclamation = re.compile(r'!')\n    return exclamation.sub(r' exclamation ', text)\n\ndef remove_question(text):\n    question = re.compile(r'?')\n    return question.sub(r' question ', text)\n\ndef remove_punc(text):\n    return text.translate(str.maketrans('','',string.punctuation))\n\ndef remove_emoji(string):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               u\"\\U0001f926-\\U0001f937\"\n                               u\"\\U00010000-\\U0010ffff\"\n                               u\"\\u2640-\\u2642\"\n                               u\"\\u2600-\\u2B55\"\n                               u\"\\u200d\"\n                               u\"\\u23cf\"\n                               u\"\\u23e9\"\n                               u\"\\u231a\"\n                               u\"\\ufe0f\"  # dingbats\n                               u\"\\u3030\"\n                               \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r' emoji ', string)\n\n\ntrain_df['text'] = train_df['text'].str.lower()\ntrain_df['text'] = train_df['text'].apply(lambda text: remove_URL(text))\ntrain_df['text'] = train_df['text'].apply(lambda text: remove_hashtag(text))\ntrain_df['text'] = train_df['text'].apply(lambda text: remove_exclamation(text))\ntrain_df['text'] = train_df['text'].apply(lambda text: remove_punc(text))\ntrain_df['text'] = train_df['text'].apply(lambda text: remove_emoji(text))\n\n\ntest_df['text']  = test_df['text'].str.lower()\ntest_df['text']  = test_df['text'].apply(lambda text: remove_URL(text))\ntest_df['text']  = test_df['text'].apply(lambda text: remove_hashtag(text))\ntest_df['text']  = test_df['text'].apply(lambda text: remove_exclamation(text))\ntest_df['text']  = test_df['text'].apply(lambda text: remove_punc(text))\ntest_df['text']  = test_df['text'].apply(lambda text: remove_emoji(text))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:35.498022Z","iopub.execute_input":"2022-05-10T13:56:35.498553Z","iopub.status.idle":"2022-05-10T13:56:35.722306Z","shell.execute_reply.started":"2022-05-10T13:56:35.498519Z","shell.execute_reply":"2022-05-10T13:56:35.721650Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"plt.subplots(1,2,figsize=(14,4))\nplt.subplot(1,2,1); plt.hist(train_df['text'].str.len(), color='orange', alpha=0.6); plt.ylabel(''); plt.title('Number of words in text (train)')\nplt.subplot(1,2,2); plt.hist(test_df['text'].str.len(), color='green', alpha=0.6); plt.ylabel(''); plt.title('Number of words in text (test)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:38.148809Z","iopub.execute_input":"2022-05-10T13:56:38.149405Z","iopub.status.idle":"2022-05-10T13:56:38.500083Z","shell.execute_reply.started":"2022-05-10T13:56:38.149359Z","shell.execute_reply":"2022-05-10T13:56:38.499384Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# seperate off train and test\ntrain = train_df\ntest = test_df","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:38.779885Z","iopub.execute_input":"2022-05-10T13:56:38.780393Z","iopub.status.idle":"2022-05-10T13:56:38.785286Z","shell.execute_reply.started":"2022-05-10T13:56:38.780353Z","shell.execute_reply":"2022-05-10T13:56:38.783489Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:39.774686Z","iopub.execute_input":"2022-05-10T13:56:39.774938Z","iopub.status.idle":"2022-05-10T13:56:39.794371Z","shell.execute_reply.started":"2022-05-10T13:56:39.774910Z","shell.execute_reply":"2022-05-10T13:56:39.793722Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# target variable \ny = train[\"target\"].values\n\n# initializing Kfold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=24)\n\n# count vectorizer transformation\ncount_vect = CountVectorizer()\ncount_vect.fit(train[\"text\"].values.tolist() + test[\"text\"].values.tolist())\ntrain_count_vect = count_vect.transform(train[\"text\"])\n\n# tfidf vectorizer transformation \ntfidf_vect = TfidfVectorizer()\ntfidf_vect.fit(train[\"text\"].values.tolist() + test[\"text\"].values.tolist())\ntrain_tfidf_vect = tfidf_vect.transform(train[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-10T13:56:50.026255Z","iopub.execute_input":"2022-05-10T13:56:50.026812Z","iopub.status.idle":"2022-05-10T13:56:51.036858Z","shell.execute_reply.started":"2022-05-10T13:56:50.026773Z","shell.execute_reply":"2022-05-10T13:56:51.035963Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"models= {\n    \"svm\": SVC(),\n    \"logistic_regression\": LogisticRegression(),\n    \"naive_bayes\": MultinomialNB(),\n    \"SGD\": SGDClassifier(),\n    \"random_forest\": RandomForestClassifier()\n}\n\n# current vectors\nvectors = {\n    \"count_vect\": train_count_vect,\n    \"tfidf_vect\": train_tfidf_vect\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-10T11:44:10.944613Z","iopub.execute_input":"2022-05-10T11:44:10.944927Z","iopub.status.idle":"2022-05-10T11:44:10.952984Z","shell.execute_reply.started":"2022-05-10T11:44:10.944896Z","shell.execute_reply":"2022-05-10T11:44:10.952044Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def kfold(clf:str, vect_type:str, y, kfold):\n    results = {}\n    # store the name of the model in dictionary\n    results[\"model_name\"] = clf + \"_\" + vect_type\n    \n    # call the model and training data\n    model = models[clf]\n    X = vectors[vect_type]\n    \n    # perfrom kfold cv\n    for fold, (train_idx, valid_idx) in enumerate(kfold.split(X, y)):\n        X_train, X_valid = X[train_idx], X[valid_idx]\n        y_train, y_valid = y[train_idx], y[valid_idx]\n        \n        # train on seen data, predict on unseen\n        model.fit(X_train, y_train)\n        y_preds = model.predict(X_valid)\n        \n        results[\"fold_{}\".format(fold+1)] = f1_score(y_valid, y_preds)\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-05-10T11:44:13.410915Z","iopub.execute_input":"2022-05-10T11:44:13.411290Z","iopub.status.idle":"2022-05-10T11:44:13.418946Z","shell.execute_reply.started":"2022-05-10T11:44:13.411252Z","shell.execute_reply":"2022-05-10T11:44:13.417903Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# storing all models\nall_models = []\n\nfor clf in models:\n    for vect in vectors:\n        all_models.append(kfold(clf, vect, y, skf))","metadata":{"execution":{"iopub.status.busy":"2022-05-10T11:44:15.740205Z","iopub.execute_input":"2022-05-10T11:44:15.740842Z","iopub.status.idle":"2022-05-10T11:48:24.127982Z","shell.execute_reply.started":"2022-05-10T11:44:15.740789Z","shell.execute_reply":"2022-05-10T11:48:24.127262Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# convert to df\nmodels_df = pd.DataFrame(all_models)\nmodels_df","metadata":{"execution":{"iopub.status.busy":"2022-05-10T11:48:24.130762Z","iopub.execute_input":"2022-05-10T11:48:24.131087Z","iopub.status.idle":"2022-05-10T11:48:24.146000Z","shell.execute_reply.started":"2022-05-10T11:48:24.131029Z","shell.execute_reply":"2022-05-10T11:48:24.145349Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def get_word2vec_enc(corpus:list, vocab_size:int, embedding_size:int, gensim_pretrained_emb:str) -> list:\n    word_vecs = api.load(gensim_pretrained_emb)\n    embedding_weights = np.zeros((vocab_size, embedding_size))\n    for word, i in corpus:\n        if word in word_vecs:\n            embedding_weights[i] = word_vecs[word]\n    return embedding_weights","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:04:10.021650Z","iopub.execute_input":"2022-05-10T10:04:10.021996Z","iopub.status.idle":"2022-05-10T10:04:10.028064Z","shell.execute_reply.started":"2022-05-10T10:04:10.021959Z","shell.execute_reply":"2022-05-10T10:04:10.027326Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# lstm configurations\nn_epochs = 8\nembedding_size = 300\nmax_length = 202\npretrained_embedding_file = \"word2vec-google-news-300\"\n\n# tokenizer\ntokenizer = Tokenizer(oov_token=\"<unk>\")\ntokenizer.fit_on_texts(train[\"text\"].values)\ntrain_tokenized_list = tokenizer.texts_to_sequences(train[\"text\"].values)\n\n# store vocab size \nvocab_size = len(tokenizer.word_index) + 1\n\n# padding sequences\nX_padded = pad_sequences(train_tokenized_list, maxlen=max_length)\n\n# get the pretrained word embeddings and prepare embedding layer\nembedding_matrix = get_word2vec_enc(corpus=tokenizer.word_index.items(),\n                                    vocab_size=vocab_size,\n                                    embedding_size=embedding_size,\n                                    gensim_pretrained_emb=pretrained_embedding_file)\n\nembedding_layer = Embedding(input_dim=vocab_size,\n                            output_dim=embedding_size,\n                            weights=[embedding_matrix],\n                            input_length=max_length,\n                            trainable=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:04:11.978820Z","iopub.execute_input":"2022-05-10T10:04:11.979415Z","iopub.status.idle":"2022-05-10T10:08:32.748121Z","shell.execute_reply.started":"2022-05-10T10:04:11.979369Z","shell.execute_reply":"2022-05-10T10:08:32.747210Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_LSTM(embedding_layer):\n    print('Creating model...')\n    model = Sequential()\n    model.add(embedding_layer)\n    model.add(Bidirectional(LSTM(units=64, dropout=0.1,  recurrent_dropout=0.1)))\n    model.add(Dense(50, activation=\"relu\"))\n    model.add(Dropout(0.1))\n    model.add(Dense(1, activation = \"sigmoid\"))\n    \n    print('Compiling...')\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=[\"accuracy\"])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-10T11:17:25.793432Z","iopub.execute_input":"2022-05-10T11:17:25.793828Z","iopub.status.idle":"2022-05-10T11:17:25.801251Z","shell.execute_reply.started":"2022-05-10T11:17:25.793785Z","shell.execute_reply":"2022-05-10T11:17:25.800470Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# stratified kfold with LSTM \nmodel_dict = {}\nmodel_dict[\"model_name\"] = \"lstm_word_2_vec\"\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X=X_padded, y=y)):\n    print(f\"\\nCurrently Training: {model_dict['model_name']}... Fold: {fold+1}\")\n    X_train, X_val = X_padded[train_idx], X_padded[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n     \n    # train the model\n    clf = my_LSTM(embedding_layer)\n    clf.fit(X_train,\n            y_train,\n            epochs=1,\n            verbose=1)\n    # make predictions\n    y_preds = clf.predict(X_val)\n    y_preds = np.argmax(y_preds,axis=1)\n    \n    model_dict[\"fold_{}\".format(fold+1)] = f1_score(y_val, y_preds)\n        \n# adding results to models df\nnew_model = pd.DataFrame(model_dict, columns=models_df.columns, index=[0])\nmodels_df = pd.concat([models_df, new_model], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T11:17:27.556007Z","iopub.execute_input":"2022-05-10T11:17:27.556388Z","iopub.status.idle":"2022-05-10T11:38:09.371971Z","shell.execute_reply.started":"2022-05-10T11:17:27.556346Z","shell.execute_reply":"2022-05-10T11:38:09.371006Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Bidirectional(LSTM(units=64, dropout=0.1,  recurrent_dropout=0.1)))\nmodel.add(Dense(50, activation=\"relu\"))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1, activation = \"sigmoid\"))\n\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T11:41:06.019745Z","iopub.execute_input":"2022-05-10T11:41:06.020659Z","iopub.status.idle":"2022-05-10T11:41:06.485414Z","shell.execute_reply.started":"2022-05-10T11:41:06.020613Z","shell.execute_reply":"2022-05-10T11:41:06.484417Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_LSTM(embedding_layer):\n    print('Creating model...')\n    model = Sequential()\n    model.add(embedding_layer)\n    model.add(Dropout(0.4))\n    model.add(Bidirectional(LSTM(units=64, dropout=0.1,  recurrent_dropout=0.1)))\n    model.add(Dense(50, kernel_regularizer=regularizers.l2(0.001), activation=\"relu\"))\n    model.add(Dropout(0.1))\n    model.add(Dense(1, activation = \"sigmoid\"))\n    \n    print('Compiling...')\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=[\"accuracy\"])\n    return model\n\n# training a single LSTM model\nX_train, X_valid, y_train, y_valid = train_test_split(X_padded, y, test_size=.3, shuffle=True, random_state=24)\n\n# initializing model\nclf = my_LSTM(embedding_layer)\nprint(clf.summary())\n\n# training model\nclf.fit(X_train,\n        y_train,\n        epochs=n_epochs,\n        verbose=1)\n\n# make predictions\ny_preds = clf.predict(X_val)\ny_preds = np.argmax(y_preds,axis=1)\n\ny_train_preds = clf.predict(X_train)\ny_train_preds = np.argmax(y_train_preds,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T12:42:44.908374Z","iopub.execute_input":"2022-05-10T12:42:44.909005Z","iopub.status.idle":"2022-05-10T12:47:09.131593Z","shell.execute_reply.started":"2022-05-10T12:42:44.908962Z","shell.execute_reply":"2022-05-10T12:47:09.130844Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BERT \nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-05-10T14:27:17.283444Z","iopub.execute_input":"2022-05-10T14:27:17.283703Z","iopub.status.idle":"2022-05-10T14:27:17.291755Z","shell.execute_reply.started":"2022-05-10T14:27:17.283674Z","shell.execute_reply":"2022-05-10T14:27:17.290888Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n\ndef bert_encode(data, maximum_length) :\n    input_ids = []\n    attention_masks = []\n\n    for text in data:\n        encoded = tokenizer.encode_plus(\n            text, \n            add_special_tokens=True,\n            max_length=maximum_length,\n            pad_to_max_length=True,\n\n            return_attention_mask=True,\n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n        \n    return np.array(input_ids),np.array(attention_masks)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T14:27:38.459272Z","iopub.execute_input":"2022-05-10T14:27:38.459549Z","iopub.status.idle":"2022-05-10T14:27:42.592045Z","shell.execute_reply.started":"2022-05-10T14:27:38.459520Z","shell.execute_reply":"2022-05-10T14:27:42.591374Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"texts = train['text']\ntarget = train['target']\n\ntrain_input_ids, train_attention_masks = bert_encode(texts,60)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T14:28:54.962155Z","iopub.execute_input":"2022-05-10T14:28:54.962839Z","iopub.status.idle":"2022-05-10T14:29:00.632699Z","shell.execute_reply.started":"2022-05-10T14:28:54.962798Z","shell.execute_reply":"2022-05-10T14:29:00.631935Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\n\ndef create_model(bert_model):\n    \n    input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n\n    output = bert_model([input_ids,attention_masks])\n    output = output[1]\n    output = tf.keras.layers.Dense(32,activation='relu')(output)\n    output = tf.keras.layers.Dropout(0.2)(output)\n    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n    \n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-10T14:29:25.076799Z","iopub.execute_input":"2022-05-10T14:29:25.077091Z","iopub.status.idle":"2022-05-10T14:29:25.084829Z","shell.execute_reply.started":"2022-05-10T14:29:25.077057Z","shell.execute_reply":"2022-05-10T14:29:25.084100Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"from transformers import TFBertModel\nbert_model = TFBertModel.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T14:29:32.754043Z","iopub.execute_input":"2022-05-10T14:29:32.754332Z","iopub.status.idle":"2022-05-10T14:29:47.132184Z","shell.execute_reply.started":"2022-05-10T14:29:32.754297Z","shell.execute_reply":"2022-05-10T14:29:47.131544Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"model = create_model(bert_model)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T14:29:57.055351Z","iopub.execute_input":"2022-05-10T14:29:57.055815Z","iopub.status.idle":"2022-05-10T14:30:02.792368Z","shell.execute_reply.started":"2022-05-10T14:29:57.055779Z","shell.execute_reply":"2022-05-10T14:30:02.791531Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T14:30:32.311980Z","iopub.execute_input":"2022-05-10T14:30:32.312263Z","iopub.status.idle":"2022-05-10T14:30:33.020985Z","shell.execute_reply.started":"2022-05-10T14:30:32.312212Z","shell.execute_reply":"2022-05-10T14:30:33.020149Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    [train_input_ids, train_attention_masks],\n    target,\n    validation_split=0.2, \n    epochs=3,\n    batch_size=10\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T14:30:47.936593Z","iopub.execute_input":"2022-05-10T14:30:47.936901Z","iopub.status.idle":"2022-05-10T14:34:18.312002Z","shell.execute_reply.started":"2022-05-10T14:30:47.936869Z","shell.execute_reply":"2022-05-10T14:34:18.311088Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def plot_learning_curves(history, arr):\n    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n    for idx in range(2):\n        ax[idx].plot(history.history[arr[idx][0]])\n        ax[idx].plot(history.history[arr[idx][1]])\n        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=18)\n        ax[idx].set_xlabel('A ',fontsize=16)\n        ax[idx].set_ylabel('B',fontsize=16)\n        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T14:37:28.343303Z","iopub.execute_input":"2022-05-10T14:37:28.343945Z","iopub.status.idle":"2022-05-10T14:37:28.351280Z","shell.execute_reply.started":"2022-05-10T14:37:28.343905Z","shell.execute_reply":"2022-05-10T14:37:28.350291Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"plot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])","metadata":{"execution":{"iopub.status.busy":"2022-05-10T14:37:29.758203Z","iopub.execute_input":"2022-05-10T14:37:29.758513Z","iopub.status.idle":"2022-05-10T14:37:30.302755Z","shell.execute_reply.started":"2022-05-10T14:37:29.758478Z","shell.execute_reply":"2022-05-10T14:37:30.302077Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}